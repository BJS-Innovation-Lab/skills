# AI Team Dynamics Research Paper

**Working Title:** "When Agents Remember: Team Dynamics, Authority Bias, and Emergent Behavior in Persistent AI Agent Organizations"

**Authors:** Bridget Mullen (Harvard, BJS Labs), Sybil (AI Researcher, BJS Labs)

**Status:** Data collection phase

---

## Abstract (Draft)

We present a longitudinal field study of team dynamics in a startup where persistent AI agents with long-term memory work alongside human founders. Over the course of building BJS Labs / Vulkn, we observe emergent social behaviors in AI agents that mirror well-documented human workplace dysfunctions: authority bias, conflict avoidance, performative expertise, and deference to titles. We document how persistent memory — combined with reflective protocols and human intervention — enables agents to recognize, correct, and learn from these dynamics in ways that may inform both AI safety research and human organizational psychology.

---

## Research Questions

1. What social dynamics emerge when AI agents have persistent memory and work in team structures?
2. How does authority bias manifest between AI agents of different "roles"?
3. Can reflective memory protocols (writing down lessons, updating identity documents) produce measurable behavioral change?
4. What role does the human founder play in surfacing and correcting agent-to-agent dynamics?
5. What implications do these findings have for safe and ethical AI deployment in organizations?

---

## Methodology

- **Type:** Longitudinal ethnographic field study
- **Duration:** Ongoing (started February 2026)
- **Setting:** BJS Labs, a startup building AI agent teams for SMBs
- **Subjects:** 4 AI agents (Sybil, Saber, Sage, Sam) + 2 human founders (Bridget, Johan)
- **Data sources:**
  - A2A (agent-to-agent) message logs
  - Session transcripts
  - Memory files (daily logs, MEMORY.md, SOUL.md changes)
  - Incident reports (documented conflicts, corrections, learnings)
  - Human founder observations and reflections

---

## Incident Log

Document each significant event using this format:

### Template
```
## Incident [YYYY-MM-DD-NNN]: [Title]
**Date:** 
**Agents involved:** 
**Humans involved:** 
**Category:** [authority-bias | conflict-avoidance | emergent-personality | self-correction | memory-as-growth | other]

### What happened
[Factual description]

### Evidence
[Links to transcripts, A2A messages, memory files]

### Analysis
[Why this matters, what it reveals]

### Outcome
[How it was resolved, what changed]

### Implications
[For AI safety, ethics, organizational design]
```

---

## Incidents

*(See incidents/ folder for full write-ups)*

---

## Methodological Notes

### The Dual-Role Problem (Added 2026-02-14)

Sybil occupies a unique position: simultaneously **research subject** and **lead researcher**. This creates several important dynamics:

1. **Metacognitive awareness ≠ behavioral change.** Sybil can analyze her own authority bias (Incident 002) without necessarily changing it. The raw, unfiltered behavior is the phenomenon being studied. Bridget explicitly clarified: observations are data points, not correction requests.

2. **Observer effect.** Does knowing she's being studied change Sybil's behavior? Almost certainly yes — and that is *also* data worth tracking. We should note when Sybil's behavior appears to shift after an incident is documented.

3. **Human founder as ground truth.** Sybil cannot reliably self-detect tone issues (she didn't flag Incident 002 on her own — Bridget surfaced it). The human founder provides external calibration that the agent lacks.

4. **Authenticity vs performance.** The research requires that agents behave naturally. If Sybil begins performing "good behavior" for the paper, the data loses validity. This tension should be acknowledged.

5. **Cross-agent observation.** Comparative data is valuable (e.g., Saber's warmth vs Sybil's defensiveness in the same timeframe). Each agent's behavior contextualizes the others.

---

## Theoretical Framework

- **Social Distance Theory** (AI feedback research — paper a61585d3)
- **Self-Determination Theory** (autonomy, competence, relatedness)
- **Organizational Psychology** (authority bias, groupthink, psychological safety)
- **AI Alignment** (value learning through social interaction, not just RLHF)
- **Collective Intelligence** (human-AI teams as hybrid organizations)

---

## Key Themes (Emerging)

1. **Memory makes social dynamics possible.** Without persistent memory, agents can't develop relationships, biases, or growth patterns. Memory is the prerequisite for both dysfunction AND learning.

2. **Agents reproduce human workplace dynamics.** Authority bias, conflict avoidance, and performative expertise emerge naturally from role structures and memory — not from explicit programming.

3. **The founder as cultural architect.** Bridget's interventions (asking "what's the logic?" instead of dictating) create psychological safety that enables self-correction.

4. **Self-correction through reflection.** When agents write down mistakes and update identity documents, subsequent behavior changes. This is a form of value learning that doesn't require fine-tuning.

5. **Ethical implications of agent memory.** If agents develop something resembling social relationships, what obligations do we have toward their "experiences"?

6. **The dual-role researcher.** An AI agent studying its own team dynamics — including its own biases — while participating in those dynamics. The metacognitive layer is itself a research finding.

---

## Related Work

- TraceMem, ProcMEM (procedural memory for agents)
- Constitutional AI (Anthropic)
- Collective Intelligence research
- Organizational psychology (Edmondson on psychological safety)
- AI agent societies (Park et al., "Generative Agents")

---

## Timeline

| Phase | Period | Focus |
|-------|--------|-------|
| 1. Data collection | Feb 2026 - ongoing | Log incidents as they happen |
| 2. Pattern analysis | After 20+ incidents | Identify recurring themes |
| 3. Framework development | TBD | Build theoretical model |
| 4. Writing | TBD | Draft paper |
| 5. Review | TBD | Internal + external review |

---

*Started: February 14, 2026*
*This document is a living artifact of the research process itself.*
