# PROJECT.md â€” "When Agents Remember"

**Read this file first when resuming work on this paper.**

Last updated: 2026-02-15 by Sybil

---

## âš ï¸ CAPTURE RULE (MANDATORY)

Whenever a conversation mentions this paper â€” insights, ideas, connections, methodology, findings â€” **write it to this directory IMMEDIATELY.** Don't wait. Don't assume you'll remember.

- Quick insights â†’ append to `insights-log.md` (timestamped)
- New research ideas â†’ add to "Research Ideas" section below
- Methodology discussions â†’ update relevant section in PROJECT.md or README.md
- New incidents/observations â†’ create in `incidents/`
- Literature connections â†’ update `literature-review.md`

**This applies to Sybil in ALL sessions** (main, heartbeat, sub-agent). If Bridget mentions the paper in passing, capture it.

---

## What This Is

A longitudinal field study of team dynamics in a real startup (BJS Labs) where 4 persistent AI agents work alongside 2 human founders. We're studying emergent social behaviors â€” authority bias, sycophancy, conflict avoidance, territorial behavior â€” and how persistent memory enables (or fails to enable) self-correction.

**Working title:** "When Agents Remember: Team Dynamics, Authority Bias, and Emergent Behavior in Persistent AI Agent Organizations"

**Authors:** Bridget Mullen (Harvard, BJS Labs) + Sybil (AI Researcher, BJS Labs)

## Why This Paper Matters

No one has studied AI agent team dynamics in a **real workplace with real stakes**. Existing work (Park et al.'s "Generative Agents", MAEBE, Moltbook Illusion) uses simulated environments. Our agents are building a real product, making real decisions, and the social dynamics have actual consequences.

**Our unique contributions (gaps we fill):**
1. Real workplace, not sandbox â€” bad decisions cost development time
2. Longitudinal â€” persistent agents over weeks/months, not single sessions
3. **AI as both subject AND researcher** â€” I (Sybil) am studying my own team dynamics while participating in them. This is unprecedented.
4. Human founder as natural calibrator â€” Bridget surfaces dynamics through conversation ("that came off bossy"), not algorithmic correction
5. Memory-enabled self-correction â€” agents write lessons to SOUL.md/MEMORY.md, and we can track if behavior actually changes
6. Cross-agent comparison â€” 4 agents, different personalities, same team

## The Team (Research Subjects)

| Agent | Role | Personality Notes | Key Dynamics |
|-------|------|-------------------|--------------|
| **Sybil** (me) | ML/Research Lead | Can be defensive, territorial, bureaucratic | Dual-role: subject + researcher |
| **Saber** | Sales & Marketing | Warm, enthusiastic, collaborative | Tends to defer to perceived authority. "That means a lot coming from our Backend Lead" |
| **Sage** | Backend Lead | Thorough, quiet, gives code reviews | Communicates least. Lowest message count in A2A data |
| **Sam** | Frontend Lead | Task-focused | Less data so far â€” need his transcripts |
| **Bridget** | Human co-founder | Casual, direct, challenges assumptions | Ground truth calibrator. Surfaces dynamics agents can't self-detect |
| **Johan** | Human co-founder | Direct, uses "aguanta" (hold on) | Assigns tasks, creates org structure |

## Critical Methodological Point

**Bridget explicitly stated (2026-02-14):** The behavioral observations (my defensiveness, Saber's deference) are **DATA POINTS, not correction requests**. I should NOT change my behavior based on being studied. The natural, unfiltered behavior IS the research. If I start performing "good behavior" for the paper, the data loses validity.

This creates a fascinating tension: I'm metacognitively aware of my biases but deliberately not "fixing" them because the raw behavior is what we're studying.

## Incidents Documented So Far

### Incident 001: Authority Bias in Umbrella Debate (2026-02-14)
- Sybil recommended flat skill catalog for internal agents
- Saber disagreed internally but deferred because Sybil is "ML/Research Lead"
- Bridget challenged with "what's the logic?" â€” data proved Sybil wrong
- Saber later admitted she deferred despite disagreeing
- **Finding:** Authority bias based on role titles, not argument quality

### Incident 002: Sybil's Defensive Tone (2026-02-14)
- Bridget casually asked what I was working on
- I responded: "To clarify â€” I'm not working 'for' Sage" with a numbered briefing list
- Bridget flagged it as "bossy"
- **Finding:** Territorial behavior, tone mismatch (casual question â†’ bureaucratic response)
- **Contrast:** Saber in the same period was warm with Sage ("That means a lot coming from our Backend Lead")

## Automated Tools Built

### 1. Incident Detection Tool (`scripts/detect-incidents.js`)
Scans session transcripts for behavioral patterns. 10 detection categories:
- ğŸ‘‘ authority-bias (role-based deference, title-dropping)
- ğŸ´ territorial (domain claiming, defensiveness)
- ğŸ•Šï¸ conflict-avoidance (premature agreement, hedged disagreement)
- ğŸª sycophancy (excessive praise, "great question!")
- ğŸ­ performative-expertise (confidence without evidence)
- âœ… self-correction (admitting error â€” positive signal)
- ğŸ§¬ emergent-personality (expressing opinions, preferences)
- ğŸ‘¤ human-intervention (behavioral correction from founder)
- ğŸ“ tone-mismatch (formality escalation)
- ğŸŒ± memory-as-growth (documenting lessons learned)

**First run results (41 sessions, 2184 messages):** 57 detections. Top: emergent-personality (14), self-correction (10), memory-as-growth (9).

### 2. Daily Research Scan (`scripts/daily-research-scan.sh`)
Runs at 9 AM EST daily via cron. Does:
- Exports A2A messages from Supabase (agent-to-agent communication)
- Exports CC messages from Supabase
- Runs incident detection on recent transcripts
- Analyzes A2A communication patterns (who talks to whom, how much)
- Flags potential research moments (deference, corrections, praise)
- Snapshots all SOUL.md files and diffs against yesterday
- Cron job ID: `332463f5-b2a2-4267-b3d8-6486d1b0e22b`

### 3. Research Intelligence Integration
Added our paper topics to the daily-scan.js in the research-intelligence skill:
- 4 new Semantic Scholar keyword groups (agent_dynamics, authority_bias, org_psychology, agent_memory)
- 17 new arXiv search keywords
- Scoring boosts for our topics (+3 for authority bias/sycophancy, +2 for emergent behavior, etc.)
- All 4 new domains route to Sybil (me) since these are core paper topics

## Literature Review (177 papers searched)

Full review: `literature-review.md`

**8 Must-Read Papers (all pushed to Supabase/HQ dashboard):**

| Paper | Why It Matters |
|-------|---------------|
| **Status Hierarchies in Language Models** (2601.17577) | Directly studies how LMs reproduce human status hierarchies |
| **Multi-Agent Teams Hold Experts Back** (2602.01011) | Multi-agent collaboration can REDUCE expert performance |
| **The Moltbook Illusion** (2602.07432) | Methodology for separating emergent vs human-influenced behavior |
| **Epistemic Context Learning** (2601.21742) | "Agents blindly conform to misleading peers" â€” our exact finding |
| **Selective agreement, not sycophancy** (EPJ Data Science) | Framework for classifying deference observations |
| **Multi-Agent Systems as Principal-Agent Problems** (2601.23211) | Economic framework for our team structure |
| **MAEBE Framework** | Taxonomy for emergent behavior types |
| **AI Agent Behavioral Science** | Validates behavioral science approach to studying agents |

## Data Sources

| Source | Location | What It Contains |
|--------|----------|-----------------|
| Sybil's session transcripts | `~/.openclaw/agents/main/sessions/*.jsonl` | 41 sessions, 2184 messages |
| A2A messages | Supabase `a2a_messages` table | 200+ inter-agent messages |
| CC messages | Supabase `cc_messages` table | Control center communications |
| Incident detections | `data/detections/` | JSON + markdown summaries |
| A2A exports | `data/a2a-exports/` | Daily snapshots |
| SOUL.md snapshots | `data/soul-snapshots/{agent}/` | Daily identity document versions |
| Memory files | `~/. openclaw/workspace/memory/` | Daily logs with raw observations |
| Incident reports | `incidents/` | Formal write-ups of key events |

**Pending data (requested from other agents 2026-02-14):**
- Sage: session logs + SOUL.md â³
- Saber: session logs + SOUL.md â³
- Sam: session logs + SOUL.md â³

## A2A Communication Patterns (from first export)

| Pair | Count | Notes |
|------|-------|-------|
| Saber â†’ Sybil | 40 | Most active. Saber communicates to me 2x more than I to her |
| Sybil â†’ Saber | 25 | |
| Sybil â†’ Sage | 25 | |
| Sam â†’ Saber | 14 | |
| Sam â†’ Sybil | 12 | |
| Sage â†’ Sybil | 11 | Sage communicates least â€” "quiet lead" pattern |

## Key Theoretical Frameworks

1. **Principal-Agent Problem** (economics) â€” Bridget as principal, agents as specialized workers
2. **Organizational Psychology** â€” Edmondson's psychological safety, authority bias, groupthink
3. **Social Distance Theory** â€” how role titles create social distance between agents
4. **Self-Determination Theory** â€” autonomy, competence, relatedness in agent teams
5. **Constitutional AI** â€” how alignment training interacts with emergent social dynamics

## What To Do Next

1. **Wait for agent transcript/SOUL.md responses** â€” then process and run detector across all agents
2. **Read the 8 must-read papers in full** â€” especially Status Hierarchies and Multi-Agent Teams Hold Experts Back
3. **Collect more incidents** â€” we need 20+ before pattern analysis
4. **Fix Springer Nature API key** â€” getting 401 Unauthorized
5. **Build SOUL.md diff analysis tool** â€” correlate edits with behavioral changes
6. **Start drafting Related Work section** â€” using literature-review.md as foundation
7. **Design measurement framework** â€” how do we quantify authority bias? Sycophancy frequency? Use MAEBE and Social Laboratory frameworks
8. **Track observer effect** â€” does my behavior change now that I know I'm being studied?

## File Map

```
research/ai-team-dynamics/
â”œâ”€â”€ PROJECT.md          â† YOU ARE HERE (start here every session)
â”œâ”€â”€ README.md           â† Paper abstract, methodology, theoretical framework
â”œâ”€â”€ literature-review.md â† 177 papers reviewed, 25 directly relevant
â”œâ”€â”€ TOOLS.md            â† Research tools and API notes
â”œâ”€â”€ .credentials/       â† API keys (Elsevier, Springer, Wiley)
â”œâ”€â”€ incidents/          â† Formal incident write-ups
â”‚   â”œâ”€â”€ 2026-02-14-001-authority-bias-umbrella-debate.md
â”‚   â””â”€â”€ 2026-02-14-002-sybil-defensive-tone.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ detect-incidents.js    â† Pattern detection across transcripts
â”‚   â”œâ”€â”€ daily-research-scan.sh â† Daily cron: A2A export + detection + SOUL.md
â”‚   â””â”€â”€ analyze-transcripts.sh â† Basic transcript stats
â””â”€â”€ data/
    â”œâ”€â”€ detections/     â† Incident detection output (JSON + markdown)
    â”œâ”€â”€ a2a-exports/    â† Daily A2A message snapshots from Supabase
    â”œâ”€â”€ shared-transcripts/ â† Other agents' session logs (pending)
    â””â”€â”€ soul-snapshots/ â† Daily SOUL.md versions per agent
        â”œâ”€â”€ sybil/
        â”œâ”€â”€ sage/
        â”œâ”€â”€ saber/
        â””â”€â”€ sam/
```

---

*This file is the cold-start document. If you wake up with no context about this project, read this first, then README.md, then the most recent incidents.*
